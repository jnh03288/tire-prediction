{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os, errno\n",
    "import sys\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "#torch.manual_seed(101)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(101)\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "    \n",
    "    \n",
    "class lstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 3):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device='cuda'),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device='cuda'))\n",
    "\n",
    "\n",
    "class lstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers = 3):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
    "        output = self.linear(lstm_out.squeeze(0))     \n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "class lstm_seq2seq(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of expected features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        '''\n",
    "\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size).cuda()\n",
    "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size).cuda()\n",
    "        \n",
    "\n",
    "    def train_model(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, training_prediction = 'recursive', teacher_forcing_ratio = 0.5, learning_rate = 0.01, dynamic_tf = False):\n",
    "        \n",
    "        '''\n",
    "        train lstm encoder-decoder\n",
    "        \n",
    "        : param input_tensor:              input data with shape (seq_len, # in batch, number features); PyTorch tensor    \n",
    "        : param target_tensor:             target data with shape (seq_len, # in batch, number features); PyTorch tensor\n",
    "        : param n_epochs:                  number of epochs \n",
    "        : param target_len:                number of values to predict \n",
    "        : param batch_size:                number of samples per gradient update\n",
    "        : param training_prediction:       type of prediction to make during training ('recursive', 'teacher_forcing', or\n",
    "        :                                  'mixed_teacher_forcing'); default is 'recursive'\n",
    "        : param teacher_forcing_ratio:     float [0, 1) indicating how much teacher forcing to use when\n",
    "        :                                  training_prediction = 'teacher_forcing.' For each batch in training, we generate a random\n",
    "        :                                  number. If the random number is less than teacher_forcing_ratio, we use teacher forcing.\n",
    "        :                                  Otherwise, we predict recursively. If teacher_forcing_ratio = 1, we train only using\n",
    "        :                                  teacher forcing.\n",
    "        : param learning_rate:             float >= 0; learning rate\n",
    "        : param dynamic_tf:                use dynamic teacher forcing (True/False); dynamic teacher forcing\n",
    "        :                                  reduces the amount of teacher forcing for each epoch\n",
    "        : return losses:                   array of loss function for each epoch\n",
    "        '''\n",
    "        \n",
    "        #\n",
    "        \n",
    "        # initialize array of losses \n",
    "        losses = np.full(n_epochs, np.nan)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        criterion = nn.MSELoss().cuda()\n",
    "        \n",
    "        # calculate number of batch iterations\n",
    "        n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "\n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                \n",
    "                batch_loss = 0.\n",
    "                batch_loss_tf = 0.\n",
    "                batch_loss_no_tf = 0.\n",
    "                num_tf = 0\n",
    "                num_no_tf = 0\n",
    "\n",
    "                for b in range(n_batches):\n",
    "                    # select data \n",
    "                    input_batch = input_tensor[:, b: b + batch_size, :].cuda()\n",
    "                    target_batch = target_tensor[:, b: b + batch_size, :].cuda()\n",
    "                    # outputs tensor\n",
    "                    outputs = torch.zeros(target_len, batch_size, input_batch.shape[2])\n",
    "\n",
    "                    # initialize hidden state\n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
    "\n",
    "                    # zero the gradient\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # encoder outputs\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "\n",
    "                    # decoder with teacher forcing\n",
    "                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "                    decoder_hidden = encoder_hidden\n",
    "\n",
    "                    if training_prediction == 'recursive':\n",
    "                        # predict recursively\n",
    "                        for t in range(target_len): \n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'teacher_forcing':\n",
    "                        # use teacher forcing\n",
    "                        if random.random() < teacher_forcing_ratio:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "\n",
    "                        # predict recursively \n",
    "                        else:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'mixed_teacher_forcing':\n",
    "                        # predict using mixed teacher forcing\n",
    "                        for t in range(target_len):\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            \n",
    "                            # predict with teacher forcing\n",
    "                            if random.random() < teacher_forcing_ratio:\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "                            \n",
    "                            # predict recursively \n",
    "                            else:\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    # compute the loss \n",
    "                    loss = criterion(outputs.cuda(), target_batch.cuda())\n",
    "                    batch_loss += loss.item()\n",
    "                    \n",
    "                    # backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # loss for epoch \n",
    "                batch_loss /= n_batches \n",
    "                losses[it] = batch_loss\n",
    "\n",
    "                # dynamic teacher forcing\n",
    "                if dynamic_tf and teacher_forcing_ratio > 0:\n",
    "                    teacher_forcing_ratio = teacher_forcing_ratio - 0.02 \n",
    "\n",
    "                # progress bar \n",
    "                tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "                    \n",
    "        return losses\n",
    "\n",
    "    def predict(self, input_tensor, target_len):\n",
    "        \n",
    "        '''\n",
    "        : param input_tensor:      input data (seq_len, input_size); PyTorch tensor \n",
    "        : param target_len:        number of target values to predict \n",
    "        : return np_outputs:       np.array containing predicted values; prediction done recursively \n",
    "        '''\n",
    "\n",
    "        # encode input_tensor\n",
    "        input_tensor = input_tensor.unsqueeze(1)     # add in batch size of 1\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
    "\n",
    "        # initialize tensor for predictions\n",
    "        outputs = torch.zeros(target_len, input_tensor.shape[2])\n",
    "\n",
    "        # decode input_tensor\n",
    "        decoder_input = input_tensor[-1, :, :]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output.squeeze(0)\n",
    "            decoder_input = decoder_output\n",
    "            \n",
    "        np_outputs = outputs.detach().numpy()\n",
    "        \n",
    "        return np_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_feather(path):\n",
    "    return pd.read_feather(path)\n",
    "\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def remove_max(x):\n",
    "    x[x.argmax()] = np.median(x)\n",
    "    print(x.argmax(), \":\", x[x.argmax()], \"=>\", np.median(x))\n",
    "    return x\n",
    "\n",
    "def groupby_datapoint(df,\n",
    "                      gb='YYYYMMDD',\n",
    "                      target='Qty'):\n",
    "    \n",
    "    df_ts = df.groupby([gb])[target].sum().sort_index()\n",
    "    df_ts = df_ts.reset_index()\n",
    "    #df_ts[gb] = pd.to_datetime(df_ts[gb], format='%Y%m%d')\n",
    "    df_ts[gb] = df_ts[gb].astype(int)\n",
    "\n",
    "    df_ts.set_index([gb], inplace=True)\n",
    "    df_ts.sort_index(inplace=True)\n",
    "    return df_ts \n",
    "\n",
    "\n",
    "def windowed_dataset(y, input_window = 5, output_window = 1, stride = 1, num_features = 1):\n",
    "  \n",
    "    '''\n",
    "    create a windowed dataset\n",
    "    \n",
    "    : param y:                time series feature (array)\n",
    "    : param input_window:     number of y samples to give model \n",
    "    : param output_window:    number of future y samples to predict  \n",
    "    : param stide:            spacing between windows   \n",
    "    : param num_features:     number of features (i.e., 1 for us, but we could have multiple features)\n",
    "    : return X, Y:            arrays with correct dimensions for LSTM\n",
    "    :                         (i.e., [input/output window size # examples, # features])\n",
    "    '''\n",
    "  \n",
    "    L = y.shape[0]\n",
    "    num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "    X = np.zeros([input_window, num_samples, num_features])\n",
    "    Y = np.zeros([output_window, num_samples, num_features])    \n",
    "    \n",
    "    for ff in np.arange(num_features):\n",
    "        for ii in np.arange(num_samples):\n",
    "            start_x = stride * ii\n",
    "            end_x = start_x + input_window\n",
    "            X[:, ii, ff] = y[start_x:end_x, ff]\n",
    "\n",
    "            start_y = stride * ii + input_window\n",
    "            end_y = start_y + output_window \n",
    "            Y[:, ii, ff] = y[start_y:end_y, ff]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def numpy_to_torch(x):\n",
    "    return torch.from_numpy(x).type(torch.Tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DATA_PATH = 'Tire_edit.csv'\n",
    "    MODEL_PATH = 'tire.pth'\n",
    "    RESULT_PATH = 'sample_submission.csv'\n",
    "\n",
    "    EPOCHS = 85\n",
    "    BATCH_SIZE = 512\n",
    "    LR = 0.005\n",
    "   \n",
    "    IN_WINDOW_SIZE = 7*4\n",
    "    OUT_WINDOW_SIZE = 31\n",
    "    STRIDE = 1\n",
    "    NUM_FEATURE = 1\n",
    "    TARGET_COL = 'Qty'    \n",
    "    \n",
    "    HIDDEN_SIZE = 128\n",
    "    \n",
    "    # Load data\n",
    "    train_df = load_csv(DATA_PATH)\n",
    "    train_df = train_df.drop([train_df.columns[0]],axis=1)\n",
    "    #print(train_df.shape)\n",
    "    #train_df = train_df[:1216]\n",
    "\n",
    "    #print(train_df.iloc[1215])\n",
    "    # Preprocessing\n",
    "    train_amt = train_df[TARGET_COL].values.reshape(-1, 1)\n",
    "    #print(len(train_amt))\n",
    "    # scaling\n",
    "#     train_amt = remove_max(train_amt) # convert max value to median\n",
    "#     train_amt = remove_max(train_amt) # convert max value to median\n",
    "#     train_amt = remove_max(train_amt) # convert max value to median\n",
    "#     train_amt = remove_max(train_amt) # convert max value to median\n",
    "#     train_amt = remove_max(train_amt) # convert max value to median\n",
    "#     train_amt = remove_max(train_amt) # convert max value to median\n",
    "    scaler = MinMaxScaler().fit(train_amt)\n",
    "    train_amt = scaler.transform(train_amt)\n",
    "    #print(train_amt.shape)\n",
    "    #print(train_amt.shape)\n",
    "    # split x_train, x_test\n",
    "    x_train = train_amt[:-IN_WINDOW_SIZE] # Input for train\n",
    "    x_test = train_amt[-IN_WINDOW_SIZE:]  # Input for test prediction\n",
    "    \n",
    "    # Make sequence\n",
    "    x_train, y_train = windowed_dataset(x_train,\n",
    "                                        IN_WINDOW_SIZE,\n",
    "                                        OUT_WINDOW_SIZE,\n",
    "                                        STRIDE,\n",
    "                                        NUM_FEATURE)\n",
    "\n",
    "    # to torch\n",
    "    x_train = numpy_to_torch(x_train)\n",
    "    y_train = numpy_to_torch(y_train)\n",
    "    x_test = numpy_to_torch(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = lstm_seq2seq(input_size = 1, hidden_size = HIDDEN_SIZE).cuda()\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.cuda()\n",
    "    \n",
    "    y_test_pred = model.predict(x_test.cuda(), target_len = OUT_WINDOW_SIZE)\n",
    "    #print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred1 = y_test_pred[-28:]\n",
    "y_test_pred1 = numpy_to_torch(y_test_pred1)\n",
    "y_test_pred2 = model.predict(y_test_pred1.cuda(), target_len = OUT_WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "y_test_pred3 = y_test_pred2[-29:-1]\n",
    "print(len(y_test_pred3))\n",
    "y_test_pred3 = numpy_to_torch(y_test_pred3)\n",
    "y_test_pred4 = model.predict(y_test_pred3.cuda(), target_len = OUT_WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = scaler.inverse_transform(y_test_pred)\n",
    "y_test_pred2 = scaler.inverse_transform(y_test_pred2)\n",
    "y_test_pred4 = scaler.inverse_transform(y_test_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_test_pred.tolist()\n",
    "y_test_pred2 = y_test_pred2.tolist()\n",
    "y_test_pred4 = y_test_pred4.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_test_pred[26]\n",
    "del y_test_pred[25]\n",
    "del y_test_pred[19]\n",
    "del y_test_pred[18]\n",
    "del y_test_pred[12]\n",
    "del y_test_pred[11]\n",
    "del y_test_pred[5]\n",
    "\n",
    "\n",
    "del y_test_pred2[30]\n",
    "del y_test_pred2[23]\n",
    "del y_test_pred2[16]\n",
    "del y_test_pred2[9]\n",
    "del y_test_pred2[8]\n",
    "del y_test_pred2[2]\n",
    "del y_test_pred2[1]\n",
    "\n",
    "del y_test_pred4[28]\n",
    "del y_test_pred4[27]\n",
    "del y_test_pred4[25]\n",
    "del y_test_pred4[24]\n",
    "del y_test_pred4[23]\n",
    "del y_test_pred4[21]\n",
    "del y_test_pred4[20]\n",
    "del y_test_pred4[14]\n",
    "del y_test_pred4[13]\n",
    "del y_test_pred4[7]\n",
    "del y_test_pred4[6]\n",
    "del y_test_pred4[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "finalout =[]\n",
    "\n",
    "finalout.extend(y_test_pred)\n",
    "finalout.extend(y_test_pred2)\n",
    "finalout.extend(y_test_pred4)\n",
    "\n",
    "finalout = sum(finalout, [])\n",
    "\n",
    "\n",
    "print(len(finalout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    YYYYMMDD           Qty\n",
      "0   20191001  27969.736328\n",
      "1   20191002  22318.718750\n",
      "2   20191003     50.000000\n",
      "3   20191004  23526.933594\n",
      "4   20191005     50.000000\n",
      "..       ...           ...\n",
      "62  20191220   4195.848926\n",
      "63  20191223   4087.154297\n",
      "64  20191227   4008.524414\n",
      "65  20191230   4062.691406\n",
      "66  20191231   3513.066797\n",
      "\n",
      "[67 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "    sub_sample = pd.read_csv(\"/home/workspace/data/.train/.task152/submission_sample.csv\")\n",
    "    len_ans = sub_sample.shape[0]\n",
    "    sub_sample['Qty'] = finalout[:len_ans]\n",
    "#     for qwe in range(0,23):\n",
    "#         sub_sample.iloc[qwe,1] = sub_sample.iloc[qwe,1]\n",
    "        \n",
    "    for qwe in range(52,67):\n",
    "        sub_sample.iloc[qwe,1] = sub_sample.iloc[qwe,1]*0.6\n",
    "        \n",
    "    for qwe in range(58,67):\n",
    "        sub_sample.iloc[qwe,1] = sub_sample.iloc[qwe,1]*0.5\n",
    "    \n",
    "    for qwe in range(62,67):\n",
    "        sub_sample.iloc[qwe,1] = sub_sample.iloc[qwe,1]*0.5\n",
    "        \n",
    "    sub_sample.iloc[2,1] = 50\n",
    "    sub_sample.iloc[4,1] = 50\n",
    "    sub_sample.iloc[35,1] = 50\n",
    "    sub_sample.iloc[41,1] = 50\n",
    "    sub_sample.iloc[47,1] = 50   \n",
    "    #sub_sample.iloc[23,1] = sub_sample.iloc[23,1]*1.2\n",
    "    \n",
    "    sub_sample.to_csv(RESULT_PATH, index=False)\n",
    "    \n",
    "    print(sub_sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is file:  True\n"
     ]
    }
   ],
   "source": [
    "from nipa.taskSubmit import nipa_submit\n",
    "import os\n",
    "\n",
    "team_id = \"1327\"\n",
    "task_no= \"152\"\n",
    "prediction_path = RESULT_PATH\n",
    "# 파일 존재 여부 확인\n",
    "print(\"is file: \", os.path.isfile(prediction_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201126221555856783_mdUG.csv: 200\n"
     ]
    }
   ],
   "source": [
    "# 제출 성공\n",
    "nipa_submit(team_id=team_id,\n",
    "task_no=task_no,\n",
    "result=prediction_path\n",
    "model = \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
